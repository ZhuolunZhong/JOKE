{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!python --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "owEINbJwtWGG",
        "outputId": "6c525820-a3a1-419d-f33f-fdb3aab77c29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.10.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install packages"
      ],
      "metadata": {
        "id": "980HtElpI6Kk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "aOLaLb-CCeNc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b65a931a-7eca-48f3-cb6c-f07ec4fd5660"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m51.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.6/190.6 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m66.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.5/44.5 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.6/31.6 MB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m409.8/409.8 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-metadata 1.14.0 requires protobuf<4.21,>=3.20.3, but you have protobuf 4.21.12 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "! pip install streamlit -q\n",
        "! pip install openai -q\n",
        "! pip install langchain -q\n",
        "! pip install pandas -q\n",
        "! pip install mysql-connector-python -q"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creat the \"endpoint IP\" needed for webpage, copy the shown IP"
      ],
      "metadata": {
        "id": "Bb8q84qhJMoR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q -O - ipv4.icanhazip.com"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2985BcnVCihj",
        "outputId": "456ef226-b405-4836-e981-db3584b468b9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34.139.77.21\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create the streamlit app file"
      ],
      "metadata": {
        "id": "B9FN5hibJpRi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "\n",
        "import streamlit as st\n",
        "import random\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.memory.chat_message_histories import StreamlitChatMessageHistory\n",
        "from langchain import OpenAI, ConversationChain\n",
        "import mysql.connector\n",
        "from mysql.connector import Error\n",
        "import pandas as pd\n",
        "from pandas import DataFrame, read_csv, read_sql_query\n",
        "\n",
        "st.set_page_config(layout=\"wide\")\n",
        "if 'caption' not in st.session_state:\n",
        "    st.session_state.caption = []\n",
        "def create_connection():\n",
        "    try:\n",
        "        connection = mysql.connector.connect(host='dbnewyorkcartoon.cgyqzvdc98df.us-east-2.rds.amazonaws.com',\n",
        "                                             database='new_york_cartoon',\n",
        "                                             user='dbuser',\n",
        "                                             password='Sql123456')\n",
        "        if connection.is_connected():\n",
        "            db_Info = connection.get_server_info()\n",
        "            print(\"Connected to MySQL Server version \", db_Info)\n",
        "            return connection\n",
        "    except Error as e:\n",
        "        print(\"Error while connecting to MySQL\", e)\n",
        "        return None\n",
        "\n",
        "def select(q):\n",
        "    connection = create_connection()\n",
        "    if connection:\n",
        "        try:\n",
        "            df = read_sql_query(q, connection)\n",
        "            return df\n",
        "        except Error as e:\n",
        "            print(\"Error executing the query:\", e)\n",
        "        finally:\n",
        "            connection.close()\n",
        "    return None\n",
        "\n",
        "col1, col2 = st.columns(2)\n",
        "with col1:\n",
        "    form = st.form(\"my_form\")\n",
        "    GPT_API = form.text_input('Your GPT API:', key=\"API\", type=\"password\")\n",
        "    form.form_submit_button(\"Submit\")\n",
        "    option_function = st.selectbox('Choose a function', ('Inspiration', 'Get Help from GPT', 'Funniness prediction','topic model graph'))\n",
        "\n",
        "with col2:\n",
        "    if option_function =='Inspiration' and st.button(':red[Give me some inspiration]'):\n",
        "        random_img = random.randint(509, 853)\n",
        "        random_integers = [random.randint(0, 19) for _ in range(3)]\n",
        "        inspiration_URL = select(\"\"\"SELECT image_url FROM base\"\"\")\n",
        "        inspiration_caption = select(f\"\"\"SELECT caption FROM result WHERE contest_num={random_img}\"\"\")\n",
        "        st.image(inspiration_URL.iloc[random_img+1, 0])\n",
        "        st.write(inspiration_caption.iloc[random_integers[0],0])\n",
        "        st.write(inspiration_caption.iloc[random_integers[1],0])\n",
        "        st.write(inspiration_caption.iloc[random_integers[2],0])\n",
        "\n",
        "    if option_function =='Get Help from GPT':\n",
        "        if not st.session_state.API:\n",
        "            st.title(':red[You need to enter the API!]')\n",
        "        else:\n",
        "            msgs = StreamlitChatMessageHistory(key=\"langchain_messages\")\n",
        "            memory = ConversationBufferMemory(chat_memory=msgs)\n",
        "            template = \"\"\"You are an AI chatbot having a conversation with a human.\n",
        "\n",
        "            {history}\n",
        "            Human: {human_input}\n",
        "            AI: \"\"\"\n",
        "            prompt = PromptTemplate(input_variables=[\"history\", \"human_input\"], template=template)\n",
        "            prompt_remember = PromptTemplate(\n",
        "                input_variables=[\"description\"],\n",
        "                template=\"You are assisting someone trying to think of funny captions for a New York cartoon. Please remember this scene as the description of a cartoon: {description}\")\n",
        "            prompt_caption = PromptTemplate(\n",
        "                input_variables=[\"caption\"],\n",
        "                template=\"Please remember this caption fot the cartoon: {caption}\")\n",
        "            prompt_suggestion = PromptTemplate(\n",
        "                input_variables=[\"suggestion\"],\n",
        "                template=\"The user wants some more advice. One way of making a cartoon funnier is to {suggestion} With that in mind, please talk directly to the user to suggest ways the user can adapt their ideas to create a funnier caption. Respond in no more than three sentences.\")\n",
        "            llm_chain = LLMChain(llm=OpenAI(openai_api_key=st.session_state.API), prompt=prompt, memory=memory)\n",
        "\n",
        "            with st.form(key='my_form2'):\n",
        "                description_location = st.text_area('Please describe the content of the cartoon in as much detail as possible:', height = 2)\n",
        "                submit_button_descriotion = st.form_submit_button(label='Submit the description of the cartoon')\n",
        "            with st.form(key='my_form3'):\n",
        "                caption_1 = st.text_input('Write your caption:')\n",
        "                submit_button_caption = st.form_submit_button(label='Submit the caption')\n",
        "            with st.form(key=\"my_form4\"):\n",
        "                option_help = st.selectbox('Choose a kind of help to ask GPT', ('use more specific language', 'make reference to all important elements of the image', 'use langauge with more than one interpretation','avoid making the joke too obvious'))\n",
        "                help_button = st.form_submit_button(label='Get help')\n",
        "            if st.button('Reset chat'):\n",
        "                del st.session_state.langchain_messages\n",
        "                msgs = StreamlitChatMessageHistory(key=\"langchain_messages\")\n",
        "\n",
        "            if submit_button_caption:\n",
        "                st.session_state.caption.append(caption_1)\n",
        "                prompt = prompt_caption.format(caption=caption_1)\n",
        "                response = llm_chain.run(prompt)\n",
        "                st.chat_message(\"ai\").write(response)\n",
        "            if submit_button_descriotion:\n",
        "                prompt = prompt_remember.format(description=description_location)\n",
        "                response = llm_chain.run(prompt)\n",
        "                st.chat_message(\"ai\").write(response)\n",
        "            if help_button:\n",
        "                prompt = prompt_suggestion.format(suggestion=option_help)\n",
        "                response = llm_chain.run(prompt)\n",
        "                st.chat_message(\"ai\").write(response)\n",
        "\n",
        "    if option_function =='Funniness prediction':\n",
        "        st.title('Wait for further development!')\n",
        "\n",
        "    if option_function =='topic model graph':\n",
        "        st.title('Wait for further development!')\n",
        "\n",
        "if option_function =='Get Help from GPT':\n",
        "  with col1:\n",
        "    with st.expander(\"View your submitted captions:\"):\n",
        "      if st.button(\"clear caption history\"):\n",
        "        st.session_state.caption = []\n",
        "      st.write(st.session_state.caption)\n",
        "  with col2:\n",
        "    with st.expander(\"View suggestion history:\"):\n",
        "      count = int(len(msgs.messages)/2)\n",
        "      for i in range(count):\n",
        "          st.chat_message(msgs.messages[i*2-1].type).write(msgs.messages[i*2-1].content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uejRR4BKCi-Q",
        "outputId": "253dc4f9-efe9-420d-c5c1-f9669dd5393e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run the streamlit app. Keep this line of codes running and paste the endpoint IP to the url shown in the button followed \"npx: installed\""
      ],
      "metadata": {
        "id": "I0k__7DfJvqr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! streamlit run app.py & npx localtunnel --port 8501"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sViSevyqCjgn",
        "outputId": "9de2c263-4c15-4912-ef60-b547f5c2c151"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[..................] \\ rollbackFailedOptional: verb npm-session 4382c83a1f584e5\u001b[0m\u001b[K\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to False.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.139.77.21:8501\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[K\u001b[?25hnpx: installed 22 in 7.959s\n",
            "your url is: https://eleven-colts-notice.loca.lt\n",
            "/usr/local/lib/python3.10/dist-packages/langchain/__init__.py:34: UserWarning: Importing OpenAI from langchain root module is no longer supported. Please use langchain.llms.OpenAI instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain/__init__.py:34: UserWarning: Importing ConversationChain from langchain root module is no longer supported. Please use langchain.chains.ConversationChain instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain/__init__.py:34: UserWarning: Importing OpenAI from langchain root module is no longer supported. Please use langchain.llms.OpenAI instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain/__init__.py:34: UserWarning: Importing ConversationChain from langchain root module is no longer supported. Please use langchain.chains.ConversationChain instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain/__init__.py:34: UserWarning: Importing OpenAI from langchain root module is no longer supported. Please use langchain.llms.OpenAI instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain/__init__.py:34: UserWarning: Importing ConversationChain from langchain root module is no longer supported. Please use langchain.chains.ConversationChain instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain/__init__.py:34: UserWarning: Importing OpenAI from langchain root module is no longer supported. Please use langchain.llms.OpenAI instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain/__init__.py:34: UserWarning: Importing ConversationChain from langchain root module is no longer supported. Please use langchain.chains.ConversationChain instead.\n",
            "  warnings.warn(\n",
            "\u001b[34m  Stopping...\u001b[0m\n",
            "^C\n"
          ]
        }
      ]
    }
  ]
}